---
title: "Discovery in social media text"
author: Pablo Barbera
date: August 10, 2018
output: html_document
---

## Exploring large-scale text datasets

When faced with a new corpus of social media text whose characteristics are unknown, it's a good idea to start by conducting some descriptive analysis to understand how documents are similar or different.

Let's learn some of those techniques with our previous example containing tweets by the four leading candidates in the 2016 Presidential primaries.

```{r}
library(quanteda)
tweets <- read.csv("~/data/candidate-tweets.csv", stringsAsFactors=F)
# extract month data and subset only data during campaign
tweets$month <- substr(tweets$datetime, 1, 7)
tweets <- tweets[tweets$month>"2015-06",]
# create DFM at the candidate and month level
twcorpus <- corpus(tweets)
```

### Identifying most unique features of documents

_Keyness_ is a measure of to what extent some features are specific to a (group of) document in comparison to the rest of the corpus, taking into account that some features may be too rare.

```{r, eval=FALSE}
twdfm <- dfm(twcorpus, groups=c("screen_name"), verbose=TRUE)

head(textstat_keyness(twdfm, target="realDonaldTrump",
                      measure="chi2"), n=20)
head(textstat_keyness(twdfm, target="HillaryClinton",
                      measure="chi2"), n=20)
head(textstat_keyness(twdfm, target="tedcruz",
                      measure="chi2"), n=20)
head(textstat_keyness(twdfm, target="BernieSanders",
                      measure="chi2"), n=20)


twdfm <- dfm(twcorpus, groups=c("screen_name"), remove_punct=TRUE,
             remove=c(stopwords("english"), 'rt', 'u', 's'), verbose=TRUE)
head(textstat_keyness(twdfm, target="realDonaldTrump",
                      measure="chi2"), n=20)
head(textstat_keyness(twdfm, target="HillaryClinton",
                      measure="chi2"), n=20)
head(textstat_keyness(twdfm, target="tedcruz",
                      measure="chi2"), n=20)
head(textstat_keyness(twdfm, target="BernieSanders",
                      measure="chi2"), n=20)

trump <- corpus_subset(twcorpus, screen_name=="realDonaldTrump")
twdfm <- dfm(trump, remove_punct=TRUE,
             remove=c(stopwords("english"), 'rt', 'u', 's'), verbose=TRUE)
head(textstat_keyness(twdfm, target=docvars(twdfm)$month<"2016-01", 
                      measure="chi2"), n=20)
head(textstat_keyness(twdfm, target=docvars(twdfm)$month>"2016-03", 
                      measure="chi2"), n=20)

clinton <- corpus_subset(twcorpus, screen_name=="HillaryClinton")
twdfm <- dfm(clinton, remove_punct=TRUE,
             remove=c(stopwords("english"), 'rt', 'u', 's'), verbose=TRUE)
head(textstat_keyness(twdfm, target=docvars(twdfm)$month<"2016-01", 
                      measure="chi2"), n=20)
head(textstat_keyness(twdfm, target=docvars(twdfm)$month>"2016-03", 
                      measure="chi2"), n=20)
```

We can use `textplot_xray` to visualize where some words appear in the corpus.

```{r}
agg <- aggregate(text ~ screen_name, data=tweets, FUN=paste, collapse=" ")
twcorpus <- corpus(agg)
docnames(twcorpus) <- agg$screen_name

textplot_xray(kwic(twcorpus, "hillary"), scale="relative")
textplot_xray(kwic(twcorpus, "trump"), scale="relative")
textplot_xray(kwic(twcorpus, "sanders"), scale="relative")
textplot_xray(kwic(twcorpus, "crooked"), scale="relative")
textplot_xray(kwic(twcorpus, "mexic*"), scale="relative")
textplot_xray(kwic(twcorpus, "fake"), scale="relative")
textplot_xray(kwic(twcorpus, "immigr*"), scale="relative")
textplot_xray(kwic(twcorpus, "muslim*"), scale="relative")
textplot_xray(kwic(twcorpus, "gun*"), scale="relative")

```

### Clustering documents and features

We can identify documents that are similar to one another based on the frequency of words, using `similarity`. There's different metrics to compute similarity. Here we explore two of them: [Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) and [Cosine distance](https://en.wikipedia.org/wiki/Cosine_similarity).

```{r}
twdfm <- dfm(twcorpus, groups=c("screen_name"), verbose=TRUE)
docnames(twdfm)
textstat_simil(twdfm, margin="documents", method="jaccard")
textstat_simil(twdfm, margin="documents", method="cosine")
```

And the opposite: term similarity based on the frequency with which they appear in documents:

```{r}
twdfm <- dfm(twcorpus, verbose=TRUE)
# term similarities
simils <- textstat_simil(twdfm, "america", margin="features", method="cosine")
# most similar features
df <- data.frame(
  featname = rownames(simils),
  simil = as.numeric(simils),
  stringsAsFactors=F
)
head(df[order(simils, decreasing=TRUE),], n=5)

# another example
simils <- textstat_simil(twdfm, "immigration", margin="features", method="cosine")
# most similar features
df <- data.frame(
  featname = rownames(simils),
  simil = as.numeric(simils),
  stringsAsFactors=F
)
head(df[order(simils, decreasing=TRUE),], n=5)

```

We can then used these distances to create a dendogram that can help us cluster documents.

```{r}
twdfm <- dfm(twcorpus, groups=c("screen_name"), verbose=TRUE)
# compute distances
distances <- textstat_dist(twdfm, margin="documents")
as.matrix(distances)

# clustering
cluster <- hclust(distances)
plot(cluster)
```





